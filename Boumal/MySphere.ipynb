{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My implementation of gradient descent on a sphere \n",
    "\n",
    "\n",
    "Important takeaway:\n",
    "\n",
    "To keep dimensions when multiplying vectors \n",
    "np.tensor(a,b, 0)\n",
    "\n",
    "\n",
    "for matrices matmul and dot\n",
    "\n",
    "### dot simply does the dot product\n",
    "\n",
    "A , shape(A) = (2,3,3) and \n",
    "\n",
    "B,  shape(B) = (2,3,3)\n",
    "\n",
    "np.dot(A,B) = C, shape(C) = (2,3,2,3)\n",
    "\n",
    "### Matmul function broadcasts the array like a stack of matrices \n",
    "np.matmul(A,B) = C , shape(C) = (2,3,3)\n",
    "\n",
    "So it just considers like a stack of matrices where it multiplies \"elementwise\" along first x-2 dimensions (last two indices are multiplied over)\n",
    "masically it's \n",
    "[ A[0] @ B[0], A[1] @ B[1] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numdifftools as nd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "A = np.random.normal(size=(3, 3))\n",
    "A = 0.5 * (A + A.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfcn = lambda X: -X @ A @ X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mygrad(X):\n",
    "\treturn (np.eye(3) - np.tensordot(X, np.transpose(X),0)) @ nd.Gradient(myfcn)(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "X  = np.array([1, 0, 0 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "for i in range(500):\n",
    "\tif i%100==0:\n",
    "\t\tlr = lr/10\n",
    "\tV = -lr * mygrad(X)\n",
    "\tX=X+V\n",
    "\tX = X/np.linalg.norm(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My solution is [0.78578684 0.37956989 0.48832954]\n",
      "Real is [-0.78442334 -0.38225031 -0.48843088]\n"
     ]
    }
   ],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "dominant_eigenvector = eigenvectors[:, eigenvalues.argmax()]\n",
    "print(f\"My solution is {X}\")\n",
    "print(f\"Real is {dominant_eigenvector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(X,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(2,3,3)\n",
    "b = np.random.rand(2,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.36826431, 0.3623116 , 0.23061871],\n",
       "        [0.58258805, 0.75769542, 0.58445169],\n",
       "        [0.35246349, 0.75165831, 0.64010223]],\n",
       "\n",
       "       [[0.70778062, 0.74401894, 0.62044564],\n",
       "        [0.5400433 , 0.6714374 , 0.45137049],\n",
       "        [0.26890576, 0.49733666, 0.26313891]]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36826431, 0.3623116 , 0.23061871],\n",
       "       [0.58258805, 0.75769542, 0.58445169],\n",
       "       [0.35246349, 0.75165831, 0.64010223]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0] @ b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70778062, 0.74401894, 0.62044564],\n",
       "       [0.5400433 , 0.6714374 , 0.45137049],\n",
       "       [0.26890576, 0.49733666, 0.26313891]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1] @ b[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "363cb7f1443b7970b8d885efdd93b9a61c4d451a5f0fb4b315db578a579f1c3b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('thesis': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
