
@article{zhang_grassmannian_2018,
	title = {Grassmannian {Learning}: {Embedding} {Geometry} {Awareness} in {Shallow} and {Deep} {Learning}},
	shorttitle = {Grassmannian {Learning}},
	url = {http://arxiv.org/abs/1808.02229},
	abstract = {Modern machine learning algorithms have been adopted in a range of signal-processing applications spanning computer vision, natural language processing, and artificial intelligence. Many relevant problems involve subspace-structured features, orthogonality constrained or low-rank constrained objective functions, or subspace distances. These mathematical characteristics are expressed naturally using the Grassmann manifold. Unfortunately, this fact is not yet explored in many traditional learning algorithms. In the last few years, there have been growing interests in studying Grassmann manifold to tackle new learning problems. Such attempts have been reassured by substantial performance improvements in both classic learning and learning using deep neural networks. We term the former as shallow and the latter deep Grassmannian learning. The aim of this paper is to introduce the emerging area of Grassmannian learning by surveying common mathematical problems and primary solution approaches, and overviewing various applications. We hope to inspire practitioners in different fields to adopt the powerful tool of Grassmannian learning in their research.},
	language = {en},
	urldate = {2021-09-13},
	journal = {arXiv:1808.02229 [cs, eess, math, stat]},
	author = {Zhang, Jiayao and Zhu, Guangxu and Heath Jr., Robert W. and Huang, Kaibin},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.02229},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Theory, Electrical Engineering and Systems Science - Signal Processing},
	annote = {Comment: Submitted to IEEE Signal Processing Magazine},
	file = {Zhang et al. - 2018 - Grassmannian Learning Embedding Geometry Awarenes.pdf:C\:\\Users\\VukadinoviM\\Zotero\\storage\\DXE8KXUD\\Zhang et al. - 2018 - Grassmannian Learning Embedding Geometry Awarenes.pdf:application/pdf},
}

@article{ma_flag_2020,
	title = {The flag manifold as a tool for analyzing and comparing data sets},
	url = {http://arxiv.org/abs/2006.14086},
	abstract = {The shape and orientation of data clouds reﬂect variability in observations that can confound pattern recognition systems. Subspace methods, utilizing Grassmann manifolds, have been a great aid in dealing with such variability. However, this usefulness begins to falter when the data cloud contains sufﬁciently many outliers corresponding to stray elements from another class or when the number of data points is larger than the number of features. We illustrate how nested subspace methods, utilizing ﬂag manifolds, can help to deal with such additional confounding factors. Flag manifolds, which are parameter spaces for nested subspaces, are a natural geometric generalization of Grassmann manifolds. To make practical comparisons on a ﬂag manifold, algorithms are proposed for determining the distances between points [A], [B] on a ﬂag manifold, where A and B are arbitrary orthogonal matrix representatives for [A] and [B], and for determining the initial direction of these minimal length geodesics. The approach is illustrated in the context of (hyper) spectral imagery showing the impact of ambient dimension, sample dimension, and ﬂag structure.},
	language = {en},
	urldate = {2021-11-11},
	journal = {arXiv:2006.14086 [cs, math]},
	author = {Ma, Xiaofeng and Kirby, Michael and Peterson, Chris},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.14086},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, 65F45, 62H35, 68T10, Mathematics - Optimization and Control},
	annote = {Comment: 15 pages, 8 figures},
	file = {Ma et al. - 2020 - The flag manifold as a tool for analyzing and comp.pdf:C\:\\Users\\VukadinoviM\\Zotero\\storage\\N8YZRWZR\\Ma et al. - 2020 - The flag manifold as a tool for analyzing and comp.pdf:application/pdf},
}

@article{lai_simpler_2020,
	title = {Simpler {Grassmannian} optimization},
	url = {http://arxiv.org/abs/2009.13502},
	abstract = {There are two widely used models for the Grassmannian Gr(k, n), as the set of equivalence classes of orthogonal matrices O(n)/ O(k) × O(n − k) , and as the set of trace-k projection matrices \{P ∈ Rn×n : P T = P = P 2, tr(P ) = k\}. The former, standard in manifold optimization, has the advantage of giving numerically stable algorithms but the disadvantage of having to work with equivalence classes of matrices. The latter, widely used in coding theory and probability, has the advantage of using actual matrices (as opposed to equivalence classes) but working with projection matrices is numerically unstable. We present an alternative that has both advantages and suﬀers from neither of the disadvantages; by representing k-dimensional subspaces as symmetric orthogonal matrices of trace 2k − n, we obtain Gr(k, n) ∼= \{Q ∈ O(n) : QT = Q, tr(Q) = 2k − n\}. As with the other two models, we show that diﬀerential geometric objects and operations — tangent vector, metric, normal vector, exponential map, geodesic, parallel transport, gradient, Hessian, etc — have closed-form analytic expressions that are computable with standard numerical linear algebra. In the proposed model, these expressions are considerably simpler, a result of representing Gr(k, n) as a linear section of a compact matrix Lie group O(n), and can be computed with at most one qr decomposition and one exponential of a special skew-symmetric matrix that takes only O nk(n − k) time. In particular, we completely avoid eigen- and singular value decompositions in our steepest descent, conjugate gradient, quasi-Newton, and Newton methods for the Grassmannian.},
	language = {en},
	urldate = {2021-11-11},
	journal = {arXiv:2009.13502 [math]},
	author = {Lai, Zehua and Lim, Lek-Heng and Ye, Ke},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.13502},
	keywords = {Mathematics - Optimization and Control, 14M15, 90C30, 90C53, 49Q12, 65F25, 62H12},
	annote = {Comment: 34 pages, 4 figures},
	file = {Lai et al. - 2020 - Simpler Grassmannian optimization.pdf:C\:\\Users\\VukadinoviM\\Zotero\\storage\\MKTAEQRQ\\Lai et al. - 2020 - Simpler Grassmannian optimization.pdf:application/pdf},
}

@article{adragni_grassmannoptim_2012,
	title = {{GrassmannOptim}: {An} {R} {Package} for {Grassmann} {Manifold} {Optimization}},
	volume = {50},
	copyright = {Copyright (c) 2010 Kofi Placid Adragni, R. Dennis Cook, Seongho Wu},
	issn = {1548-7660},
	shorttitle = {{GrassmannOptim}},
	url = {https://doi.org/10.18637/jss.v050.i05},
	doi = {10.18637/jss.v050.i05},
	abstract = {The optimization of a real-valued objective function f(U), where U is a p X d,p {\textgreater} d, semi-orthogonal matrix such that UTU=Id, and f is invariant under right orthogonal transformation of U, is often referred to as a Grassmann manifold optimization. Manifold optimization appears in a wide variety of computational problems in the applied sciences. In this article, we present GrassmannOptim, an R package for Grassmann manifold optimization. The implementation uses gradient-based algorithms and embeds a stochastic gradient method for global search. We describe the algorithms, provide some illustrative examples on the relevance of manifold optimization and finally, show some practical usages of the package.},
	language = {en},
	urldate = {2021-11-11},
	journal = {Journal of Statistical Software},
	author = {Adragni, Kofi Placid and Cook, R. Dennis and Wu, Seongho},
	month = jul,
	year = {2012},
	pages = {1--18},
	file = {Submitted Version:C\:\\Users\\VukadinoviM\\Zotero\\storage\\HPKT7N9L\\Adragni et al. - 2012 - GrassmannOptim An R Package for Grassmann Manifol.pdf:application/pdf},
}

@article{hamm_grassmann_nodate,
	title = {Grassmann {Discriminant} {Analysis}: a {Unifying} {View} on {Subspace}-{Based} {Learning}},
	abstract = {In this paper we propose a discriminant learning framework for problems in which data consist of linear subspaces instead of vectors. By treating subspaces as basic elements, we can make learning algorithms adapt naturally to the problems with linear invariant structures. We propose a unifying view on the subspace-based learning method by formulating the problems on the Grassmann manifold, which is the set of fixed-dimensional linear subspaces of a Euclidean space. Previous methods on the problem typically adopt an inconsistent strategy: feature extraction is performed in the Euclidean space while nonEuclidean distances are used. In our approach, we treat each subspace as a point in the Grassmann space, and perform feature extraction and classification in the same space. We show feasibility of the approach by using the Grassmann kernel functions such as the Projection kernel and the Binet-Cauchy kernel. Experiments with real image databases show that the proposed method performs well compared with state-of- the-art algorithms.},
	language = {en},
	author = {Hamm, Jihun and Lee, Daniel D},
	pages = {10},
	file = {Hamm and Lee - Grassmann Discriminant Analysis a Unifying View o.pdf:C\:\\Users\\VukadinoviM\\Zotero\\storage\\TNIIRPRY\\Hamm and Lee - Grassmann Discriminant Analysis a Unifying View o.pdf:application/pdf},
}

@article{he_iterative_2014,
	title = {Iterative {Grassmannian} {Optimization} for {Robust} {Image} {Alignment}},
	volume = {32},
	issn = {02628856},
	url = {http://arxiv.org/abs/1306.0404},
	doi = {10.1016/j.imavis.2014.02.015},
	abstract = {Robust high-dimensional data processing has witnessed an exciting development in recent years. Theoretical results have shown that it is possible using convex programming to optimize data ﬁt to a low-rank component plus a sparse outlier component. This problem is also known as Robust PCA, and it has found application in many areas of computer vision. In image and video processing and face recognition, the opportunity to process massive image databases is emerging as people upload photo and video data online in unprecedented volumes. However, data quality and consistency is not controlled in any way, and the massiveness of the data poses a serious computational challenge. In this paper we present t-GRASTA, or “Transformed GRASTA (Grassmannian Robust Adaptive Subspace Tracking Algorithm)”. t-GRASTA iteratively performs incremental gradient descent constrained to the Grassmann manifold of subspaces in order to simultaneously estimate three components of a decomposition of a collection of images: a low-rank subspace, a sparse part of occlusions and foreground objects, and a transformation such as rotation or translation of the image. We show that t-GRASTA is 4× faster than state-of-the-art algorithms, has half the memory requirement, and can achieve alignment for face images as well as jittered camera surveillance images.},
	language = {en},
	number = {10},
	urldate = {2021-11-11},
	journal = {Image and Vision Computing},
	author = {He, Jun and Zhang, Dejiao and Balzano, Laura and Tao, Tao},
	month = oct,
	year = {2014},
	note = {arXiv: 1306.0404},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Mathematics - Optimization and Control},
	pages = {800--813},
	annote = {Comment: Preprint submitted to the special issue of the Image and Vision Computing Journal on the theme "The Best of Face and Gesture 2013"},
	file = {He et al. - 2014 - Iterative Grassmannian Optimization for Robust Ima.pdf:C\:\\Users\\VukadinoviM\\Zotero\\storage\\92MHBWML\\He et al. - 2014 - Iterative Grassmannian Optimization for Robust Ima.pdf:application/pdf},
}

@article{johnsson_optimization_nodate,
	title = {Optimization over {Grassmann} manifolds},
	language = {en},
	author = {Johnsson, Kerstin},
	pages = {9},
  year = {2021},
	file = {Johnsson - Optimization over Grassmann manifolds.pdf:C\:\\Users\\VukadinoviM\\Zotero\\storage\\5LICNS9J\\Johnsson - Optimization over Grassmann manifolds.pdf:application/pdf},
}

@article{huang_building_2018,
	title = {Building {Deep} {Networks} on {Grassmann} {Manifolds}},
	url = {http://arxiv.org/abs/1611.05742},
	abstract = {Learning representations on Grassmann manifolds is popular in quite a few visual recognition tasks. In order to enable deep learning on Grassmann manifolds, this paper proposes a deep network architecture by generalizing the Euclidean network paradigm to Grassmann manifolds. In particular, we design full rank mapping layers to transform input Grassmannian data to more desirable ones, exploit re-orthonormalization layers to normalize the resulting matrices, study projection pooling layers to reduce the model complexity in the Grassmannian context, and devise projection mapping layers to respect Grassmannian geometry and meanwhile achieve Euclidean forms for regular output layers. To train the Grassmann networks, we exploit a stochastic gradient descent setting on manifolds of the connection weights, and study a matrix generalization of backpropagation to update the structured data. The evaluations on three visual recognition tasks show that our Grassmann networks have clear advantages over existing Grassmann learning methods, and achieve results comparable with state-of-the-art approaches.},
	language = {en},
	urldate = {2021-12-01},
	journal = {arXiv:1611.05742 [cs]},
	author = {Huang, Zhiwu and Wu, Jiqing and Van Gool, Luc},
	month = jan,
	year = {2018},
	note = {arXiv: 1611.05742},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: AAAI'18 paper},
	file = {Huang et al. - 2018 - Building Deep Networks on Grassmann Manifolds.pdf:C\:\\Users\\VukadinoviM\\Zotero\\storage\\ESUDZRQA\\Huang et al. - 2018 - Building Deep Networks on Grassmann Manifolds.pdf:application/pdf},
}

@article{tripuraneni_averaging_2018,
	title = {Averaging {Stochastic} {Gradient} {Descent} on {Riemannian} {Manifolds}},
	url = {http://arxiv.org/abs/1802.09128},
	abstract = {We consider the minimization of a function defined on a Riemannian manifold \${\textbackslash}mathcal\{M\}\$ accessible only through unbiased estimates of its gradients. We develop a geometric framework to transform a sequence of slowly converging iterates generated from stochastic gradient descent (SGD) on \${\textbackslash}mathcal\{M\}\$ to an averaged iterate sequence with a robust and fast \$O(1/n)\$ convergence rate. We then present an application of our framework to geodesically-strongly-convex (and possibly Euclidean non-convex) problems. Finally, we demonstrate how these ideas apply to the case of streaming \$k\$-PCA, where we show how to accelerate the slow rate of the randomized power method (without requiring knowledge of the eigengap) into a robust algorithm achieving the optimal rate of convergence.},
	urldate = {2021-12-01},
	journal = {arXiv:1802.09128 [cs, math, stat]},
	author = {Tripuraneni, Nilesh and Flammarion, Nicolas and Bach, Francis and Jordan, Michael I.},
	month = jun,
	year = {2018},
	note = {arXiv: 1802.09128},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
	annote = {Comment: COLT 2018},
	file = {arXiv Fulltext PDF:C\:\\Users\\VukadinoviM\\Zotero\\storage\\YRB3PKZW\\Tripuraneni et al. - 2018 - Averaging Stochastic Gradient Descent on Riemannia.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\VukadinoviM\\Zotero\\storage\\MAHBWPG4\\1802.html:text/html},
}

@article{carlini_towards_2017,
	title = {Towards {Evaluating} the {Robustness} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1608.04644},
	abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classiﬁcation t, it is possible to ﬁnd a new input x that is similar to x but classiﬁed as t. This makes it difﬁcult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks’ ability to ﬁnd adversarial examples from 95\% to 0.5\%.},
	language = {en},
	urldate = {2021-12-01},
	journal = {arXiv:1608.04644 [cs]},
	author = {Carlini, Nicholas and Wagner, David},
	month = mar,
	year = {2017},
	note = {arXiv: 1608.04644},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security},
	file = {Carlini and Wagner - 2017 - Towards Evaluating the Robustness of Neural Networ.pdf:C\:\\Users\\VukadinoviM\\Zotero\\storage\\LT5K94BE\\Carlini and Wagner - 2017 - Towards Evaluating the Robustness of Neural Networ.pdf:application/pdf},
}

@misc{noauthor_linear_2014,
	title = {Linear {Discriminant} {Analysis}},
	url = {https://sebastianraschka.com/Articles/2014_python_lda.html},
	abstract = {Sections},
	language = {en},
	urldate = {2022-01-07},
	journal = {Dr. Sebastian Raschka},
	month = aug,
	year = {2014},
	file = {Snapshot:C\:\\Users\\VukadinoviM\\Zotero\\storage\\XR5BMLIL\\2014_python_lda.html:text/html},
}

@Booklet{boumal2022intromanifolds,
  title        = {An introduction to optimization on smooth manifolds},
  author       = {Boumal, Nicolas},
  howpublished = {To appear with Cambridge University Press},
  month        = {Apr},
  year         = {2022},
  url          = {http://www.nicolasboumal.net/book},
}
@misc{fmg_data_driven_control_summer_school_introduction_2021,
	title = {An {Introduction} to {Optimization} on {Smooth} {Manifolds} -- {Nicolas} {Boumal}},
	url = {https://www.youtube.com/watch?v=lK62DwSIjXA},
	abstract = {Lecture by Nicolas Boumal as part of the Summer School "Foundations and Mathematical Guarantees of Data-Driven Control" organized by EPFL \& ETHZ.

0:00 Introduction
1:32 Start of the lecture
4:33 Classical optimization
6:40 Optimization on manifolds
10:50 What is a manifold?
23:45 Technical tools
1:32:41 Basic manifold optimization algorithm
1:41:26 The Manopt toolbox 
1:43:43 Research directions
1:51:00 Questions 

June 9th, 2021

--

Relevant work: 
Nicolas Boumal, "An Introduction to Optimization on Smooth Manifolds" 2020. 
Link to the book: http://sma.epfl.ch/{\textasciitilde}nboumal/book/inde...  
Link to the toolboxes: https://www.manopt.org/   
Link to the ManOpt forum: https://groups.google.com/g/manopttoo...},
	urldate = {2022-01-08},
	author = {{FMG Data Driven Control Summer School}},
	month = jun,
	year = {2021},
}

@misc{noauthor_manopt_nodate,
	title = {Manopt, a matlab toolbox for optimization on manifolds},
	url = {https://www.manopt.org/},
	urldate = {2022-01-08},
	file = {Manopt, a matlab toolbox for optimization on manifolds:C\:\\Users\\VukadinoviM\\Zotero\\storage\\WF9H5DV7\\www.manopt.org.html:text/html},
}

@misc{noauthor_optimization_nodate,
	title = {Optimization {Algorithms} on {Matrix} {Manifolds} - {Full} {Online} {Text}},
	url = {https://press.princeton.edu/absil},
	language = {en},
	urldate = {2022-01-09},
	file = {Snapshot:C\:\\Users\\VukadinoviM\\Zotero\\storage\\RS7LSHTF\\absil.html:text/html},
}

@article{cook_fisher_2007,
	title = {Fisher {Lecture}: {Dimension} {Reduction} in {Regression}},
	volume = {22},
	issn = {0883-4237},
	shorttitle = {Fisher {Lecture}},
	url = {http://arxiv.org/abs/0708.3774},
	doi = {10.1214/088342306000000682},
	abstract = {Beginning with a discussion of R. A. Fisher's early written remarks that relate to dimension reduction, this article revisits principal components as a reductive method in regression, develops several model-based extensions and ends with descriptions of general approaches to model-based and model-free dimension reduction in regression. It is argued that the role for principal components and related methodology may be broader than previously seen and that the common practice of conditioning on observed values of the predictors may unnecessarily limit the choice of regression methodology.},
	number = {1},
	urldate = {2022-01-11},
	journal = {Statistical Science},
	author = {Cook, R. Dennis},
	month = feb,
	year = {2007},
	note = {arXiv: 0708.3774},
	keywords = {Statistics - Methodology},
	annote = {Comment: This paper commented in: [arXiv:0708.3776], [arXiv:0708.3777], [arXiv:0708.3779]. Rejoinder in [arXiv:0708.3781]. Published at http://dx.doi.org/10.1214/088342306000000682 in the Statistical Science (http://www.imstat.org/sts/) by the Institute of Mathematical Statistics (http://www.imstat.org)},
	file = {arXiv Fulltext PDF:C\:\\Users\\VukadinoviM\\Zotero\\storage\\S9ZJTJ5H\\Cook - 2007 - Fisher Lecture Dimension Reduction in Regression.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\VukadinoviM\\Zotero\\storage\\7KMITLBC\\0708.html:text/html},
}

@article{huang_riemannian_2016,
	title = {A {Riemannian} {Network} for {SPD} {Matrix} {Learning}},
	url = {http://arxiv.org/abs/1608.04233},
	abstract = {Symmetric Positive Definite (SPD) matrix learning methods have become popular in many image and video processing tasks, thanks to their ability to learn appropriate statistical representations while respecting Riemannian geometry of underlying SPD manifolds. In this paper we build a Riemannian network architecture to open up a new direction of SPD matrix non-linear learning in a deep model. In particular, we devise bilinear mapping layers to transform input SPD matrices to more desirable SPD matrices, exploit eigenvalue rectification layers to apply a non-linear activation function to the new SPD matrices, and design an eigenvalue logarithm layer to perform Riemannian computing on the resulting SPD matrices for regular output layers. For training the proposed deep network, we exploit a new backpropagation with a variant of stochastic gradient descent on Stiefel manifolds to update the structured connection weights and the involved SPD matrix data. We show through experiments that the proposed SPD matrix network can be simply trained and outperform existing SPD matrix learning and state-of-the-art methods in three typical visual classification tasks.},
	urldate = {2022-01-15},
	journal = {arXiv:1608.04233 [cs]},
	author = {Huang, Zhiwu and Van Gool, Luc},
	month = dec,
	year = {2016},
	note = {arXiv: 1608.04233},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Revised arXiv version, AAAI-17 camera-ready},
	file = {arXiv Fulltext PDF:C\:\\Users\\VukadinoviM\\Zotero\\storage\\SMMR7T9W\\Huang and Van Gool - 2016 - A Riemannian Network for SPD Matrix Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\VukadinoviM\\Zotero\\storage\\P8E3PKWB\\1608.html:text/html},
}

@inproceedings{liu_combining_2014,
	address = {Istanbul Turkey},
	title = {Combining {Multiple} {Kernel} {Methods} on {Riemannian} {Manifold} for {Emotion} {Recognition} in the {Wild}},
	isbn = {978-1-4503-2885-2},
	url = {https://dl.acm.org/doi/10.1145/2663204.2666274},
	doi = {10.1145/2663204.2666274},
	language = {en},
	urldate = {2022-01-15},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Multimodal} {Interaction}},
	publisher = {ACM},
	author = {Liu, Mengyi and Wang, Ruiping and Li, Shaoxin and Shan, Shiguang and Huang, Zhiwu and Chen, Xilin},
	month = nov,
	year = {2014},
	pages = {494--501},
}

@article{masci_geodesic_2018,
	title = {Geodesic convolutional neural networks on {Riemannian} manifolds},
	url = {http://arxiv.org/abs/1501.06297},
	abstract = {Feature descriptors play a crucial role in a wide range of geometry analysis and processing applications, including shape correspondence, retrieval, and segmentation. In this paper, we introduce Geodesic Convolutional Neural Networks (GCNN), a generalization of the convolutional networks (CNN) paradigm to non-Euclidean manifolds. Our construction is based on a local geodesic system of polar coordinates to extract “patches”, which are then passed through a cascade of ﬁlters and linear and non-linear operators. The coefﬁcients of the ﬁlters and linear combination weights are optimization variables that are learned to minimize a task-speciﬁc cost function. We use GCNN to learn invariant shape features, allowing to achieve state-of-the-art performance in problems such as shape description, retrieval, and correspondence.},
	language = {en},
	urldate = {2022-01-15},
	journal = {arXiv:1501.06297 [cs]},
	author = {Masci, Jonathan and Boscaini, Davide and Bronstein, Michael M. and Vandergheynst, Pierre},
	month = jun,
	year = {2018},
	note = {arXiv: 1501.06297},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Masci et al. - 2018 - Geodesic convolutional neural networks on Riemanni.pdf:C\:\\Users\\VukadinoviM\\Zotero\\storage\\G7GW8T76\\Masci et al. - 2018 - Geodesic convolutional neural networks on Riemanni.pdf:application/pdf},
}

@misc{noauthor_orthogonal_nodate,
	title = {Orthogonal {Projection}},
	url = {https://textbooks.math.gatech.edu/ila/projections.html},
	urldate = {2022-01-27},
	file = {Orthogonal Projection:C\:\\Users\\VukadinoviM\\Zotero\\storage\\5TJUJ6P3\\projections.html:text/html},
}

@misc{simons_institute_constrained_2021,
	title = {Constrained {Optimization} {On} {Riemannian} {Manifolds}},
	url = {https://www.youtube.com/watch?v=SX8v-It05XU},
	abstract = {Melanie Weber (Oxford, Mathematical Institute)
https://simons.berkeley.edu/talks/con...
Optimization Under Symmetry},
	urldate = {2022-02-02},
	author = {{Simons Institute}},
	month = nov,
	year = {2021},
}

@misc{noauthor_optimization_nodate-1,
	title = {Optimization {Algorithms} on {Matrix} {Manifolds} - {Full} {Online} {Text}},
	url = {https://press.princeton.edu/absil},
	language = {en},
	urldate = {2022-02-03},
	file = {Snapshot:C\:\\Users\\VukadinoviM\\Zotero\\storage\\M65WV6ZC\\absil.html:text/html},
}

@article{damle_cs_nodate,
	title = {{CS} 3220: {Orthogonal} projectors},
	language = {en},
	author = {Damle, Anil},
	pages = {4},
	file = {Damle - CS 3220 Orthogonal projectors.pdf:C\:\\Users\\VukadinoviM\\Zotero\\storage\\QCFDRZWU\\Damle - CS 3220 Orthogonal projectors.pdf:application/pdf},
}

@misc{noauthor_optimization_nodate-2,
	title = {Optimization {Algorithms} on {Matrix} {Manifolds} - {Full} {Online} {Text}},
	url = {https://press.princeton.edu/absil},
	language = {en},
	urldate = {2022-03-08},
	file = {Snapshot:C\:\\Users\\VukadinoviM\\Zotero\\storage\\XA4UK5NX\\absil.html:text/html},
}

@misc{noauthor_differential_nodate,
	title = {differential geometry - {What} is the metric on the \$n\$-sphere in stereographic projection coordinates?},
	url = {https://math.stackexchange.com/questions/1199628/what-is-the-metric-on-the-n-sphere-in-stereographic-projection-coordinates},
	urldate = {2022-03-11},
	journal = {Mathematics Stack Exchange},
}

@article{bendokat_grassmann_2020,
	title = {A {Grassmann} {Manifold} {Handbook}: {Basic} {Geometry} and {Computational} {Aspects}},
	shorttitle = {A {Grassmann} {Manifold} {Handbook}},
	url = {http://arxiv.org/abs/2011.13699},
	abstract = {The Grassmann manifold of linear subspaces is important for the mathematical modelling of a multitude of applications, ranging from problems in machine learning, computer vision and image processing to low-rank matrix optimization problems, dynamic low-rank decompositions and model reduction. With this work, we aim to provide a collection of the essential facts and formulae on the geometry of the Grassmann manifold in a fashion that is ﬁt for tackling the aforementioned problems with matrix-based algorithms. Moreover, we expose the Grassmann geometry both from the approach of representing subspaces with orthogonal projectors and when viewed as a quotient space of the orthogonal group, where subspaces are identiﬁed as equivalence classes of (orthogonal) bases. This bridges the associated research tracks and allows for an easy transition between these two approaches.},
	language = {en},
	urldate = {2022-03-23},
	journal = {arXiv:2011.13699 [cs, math]},
	author = {Bendokat, Thomas and Zimmermann, Ralf and Absil, P.-A.},
	month = dec,
	year = {2020},
	note = {arXiv: 2011.13699},
	keywords = {15-02, 15A16, 15A18, 15B10, 22E70, 51F25, 53C80, 53Z99, Mathematics - Differential Geometry, Mathematics - Numerical Analysis},
	annote = {Comment: 35 pages, 4 figures},
	file = {Bendokat et al. - 2020 - A Grassmann Manifold Handbook Basic Geometry and .pdf:C\:\\Users\\VukadinoviM\\Zotero\\storage\\NHTRYGZU\\Bendokat et al. - 2020 - A Grassmann Manifold Handbook Basic Geometry and .pdf:application/pdf},
}

@article{helmke_newtons_2007,
	title = {Newton's method on {Gra}\{{\textbackslash}ss\}mann manifolds},
	url = {http://arxiv.org/abs/0709.2205},
	abstract = {A general class of Newton algorithms on Gra\{{\textbackslash}ss\}mann and Lagrange-Gra\{{\textbackslash}ss\}mann manifolds is introduced, that depends on an arbitrary pair of local coordinates. Local quadratic convergence of the algorithm is shown under a suitable condition on the choice of coordinate systems. Our result extends and unifies previous convergence results for Newton's method on a manifold. Using special choices of the coordinates, new numerical algorithms are derived for principal component analysis and invariant subspace computations with improved computational complexity properties.},
	language = {en},
	urldate = {2022-03-23},
	journal = {arXiv:0709.2205 [math]},
	author = {Helmke, Uwe and Hüper, Knut and Trumpf, Jochen},
	month = sep,
	year = {2007},
	note = {arXiv: 0709.2205},
	keywords = {Mathematics - Optimization and Control, Mathematics - Differential Geometry, Mathematics - Numerical Analysis, 15A18, 49M15, 53B20, 65F15},
	annote = {Comment: 36 pages, typos corrected and references added},
	file = {Helmke et al. - 2007 - Newton's method on Gra ss mann manifolds.pdf:C\:\\Users\\VukadinoviM\\Zotero\\storage\\M2639FUE\\Helmke et al. - 2007 - Newton's method on Gra ss mann manifolds.pdf:application/pdf},
}

@article{edelman_geometry_1998,
	title = {The {Geometry} of {Algorithms} with {Orthogonality} {Constraints}},
	url = {http://arxiv.org/abs/physics/9806030},
	abstract = {In this paper we develop new Newton and conjugate gradient algorithms on the Grassmann and Stiefel manifolds. These manifolds represent the constraints that arise in such areas as the symmetric eigenvalue problem, nonlinear eigenvalue problems, electronic structures computations, and signal processing. In addition to the new algorithms, we show how the geometrical framework gives penetrating new insights allowing us to create, understand, and compare algorithms. The theory proposed here provides a taxonomy for numerical linear algebra algorithms that provide a top level mathematical view of previously unrelated algorithms. It is our hope that developers of new algorithms and perturbation theories will beneﬁt from the theory, methods, and examples in this paper.},
	language = {en},
	urldate = {2022-03-25},
	journal = {arXiv:physics/9806030},
	author = {Edelman, Alan and Arias, T. A. and Smith, Steven T.},
	month = jun,
	year = {1998},
	note = {arXiv: physics/9806030},
	keywords = {Mathematics - Numerical Analysis, Condensed Matter, Physics - Chemical Physics, Physics - Computational Physics},
	annote = {Comment: The condensed matter interest is as new methods for minimizing Kohn-Sham orbitals under the constraints of orthonormality and as "geometrically correct" generalizations and extensions of the analystically continued functional approach, Phys. Rev. Lett. 69, 1077 (1992). The problem of orthonormality constraints is quite general and the methods discussed are also applicable in a wide range of fields. To appear in SIAM Journal of Matrix Analysis and Applications, in press for sometime in August-October 1998; 52 pages, 8 figures},
	file = {Edelman et al. - 1998 - The Geometry of Algorithms with Orthogonality Cons.pdf:C\:\\Users\\VukadinoviM\\Zotero\\storage\\Z55YRMR2\\Edelman et al. - 1998 - The Geometry of Algorithms with Orthogonality Cons.pdf:application/pdf},
}

@misc{noauthor_projection_nodate,
	title = {Projection {Matrix} - an overview {\textbar} {ScienceDirect} {Topics}},
	url = {https://www.sciencedirect.com/topics/mathematics/projection-matrix},
	urldate = {2022-03-29},
	file = {Projection Matrix - an overview | ScienceDirect Topics:C\:\\Users\\VukadinoviM\\Zotero\\storage\\7I6QUEFB\\projection-matrix.html:text/html},
}

@article{dimitric_grassmannians_nodate,
	title = {{GRASSMANNIANS} {VIA} {PROJECTION} {OPERATORS} {AND} {SOME} {OF} {THEIR} {SPECIAL} {SUBMANIFOLDS}},
	language = {en},
	author = {Dimitric, Ivko},
	pages = {22},
	file = {Dimitric - GRASSMANNIANS VIA PROJECTION OPERATORS AND SOME OF.pdf:C\:\\Users\\VukadinoviM\\Zotero\\storage\\QI2ZC9PV\\Dimitric - GRASSMANNIANS VIA PROJECTION OPERATORS AND SOME OF.pdf:application/pdf},
}

@misc{noauthor_riemannian_nodate,
	title = {Riemannian {Newton} iteration for {Rayleigh} quotients on the sphere},
	url = {https://www.nxn.se/valent/riemannian-newton-iteration-for-rayleigh-quotients},
	abstract = {In the above figure, each point is in initial value which will be converge to different eigenvectors of an orthogonal 3x3 matrix.
The above figure is the equivalent of a Newton fractal, but applied to Rayleigh quotient iteration on a sphere. This post will go through an explanation of the figure, an},
	language = {en-US},
	urldate = {2022-04-12},
	journal = {What do you mean "heterogeneity"?},
	file = {Snapshot:C\:\\Users\\VukadinoviM\\Zotero\\storage\\G6TVTMHJ\\riemannian-newton-iteration-for-rayleigh-quotients.html:text/html},
}

@BOOK{AbsMahSep2008,
 author = "P.-A. Absil and R. Mahony and R. Sepulchre",
 title = "Optimization Algorithms on Matrix Manifolds",
 publisher = "Princeton University Press",
 address = "Princeton, NJ",
 year = 2008,
 pages = "xvi+224",
 isbn = "978-0-691-13298-3",
 keywords = "optimization on manifolds, Riemannian optimization, retraction, vector transport",
}
@article{JMLR:v17:16-177,
    author = {James Townsend and Niklas Koep and Sebastian Weichwald},
    journal = {Journal of Machine Learning Research},
    number = {137},
    pages = {1–5},
    title = {Pymanopt: A Python Toolbox for Optimization on Manifolds using Automatic Differentiation},
    url = {http://jmlr.org/papers/v17/16-177.html},
    volume = {17},
    year = {2016}
}
@article{DBLP:journals/corr/abs-2104-13478,
  author    = {Michael M. Bronstein and
               Joan Bruna and
               Taco Cohen and
               Petar Velickovic},
  title     = {Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges},
  journal   = {CoRR},
  volume    = {abs/2104.13478},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.13478},
  eprinttype = {arXiv},
  eprint    = {2104.13478},
  timestamp = {Tue, 04 May 2021 15:12:43 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-13478.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}